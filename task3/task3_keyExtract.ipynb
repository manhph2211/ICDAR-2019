{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "task3_keyExtract.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVxACpYTZSwU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26f3938c-c402-4e9d-8da4-4e45ba1c7bb7"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')\n",
        "%cd /content/gdrive/MyDrive/FinalProject"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.activity.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fexperimentsandconfigs%20https%3a%2f%2fwww.googleapis.com%2fauth%2fphotos.native&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "4/1AY0e-g7v9KMvBJlWqphN1Gc9nRsHL_X7klsnnJC4P3HiKePtzlsMtL1A4d4\n",
            "Mounted at /content/gdrive\n",
            "/content/gdrive/MyDrive/FinalProject\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ai5dqwK7ZeB2"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "import json\n",
        "import re"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdQmD4YETRzs"
      },
      "source": [
        "# 1. Map label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heozqA_jTP2n"
      },
      "source": [
        "key_text_dir='./data/For_task_3/'\n",
        "text_dir='./data/task1_train/'\n",
        "classes={'company':1,'date':2,'address':3,'total':4,'others':0}"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iX0xAUlKT3Y_"
      },
      "source": [
        "def get_text(path):\n",
        "  text_list=[]\n",
        "  with open(path,'r') as f:\n",
        "    for line in f:\n",
        "      line=line[:-1]\n",
        "      tok=line.split(',')\n",
        "      text=','.join(tok[8:])\n",
        "      text_list.append(text)\n",
        "  return text_list\n",
        "\n",
        "\n",
        "def get_txt_file(dir_):\n",
        "  data=[]\n",
        "  for file in os.listdir(dir_):\n",
        "    if re.match('^((?!\\)).)*$',file) and file.endswith('.txt'):\n",
        "      data.append(file)  \n",
        "  return data\n",
        "\n",
        "\n",
        "def read_json(path):\n",
        "  with open(path,'r') as f:\n",
        "    dic=json.load(f)\n",
        "  return dic\n",
        "\n",
        "\n",
        "def save_json(path,data):\n",
        "  with open(path,'w') as f:\n",
        "    json.dump(data,f,indent=4)\n",
        "\n",
        "\n",
        "def encode_label(key_text_dir,text_dir):\n",
        "  txt_file=get_txt_file(text_dir)\n",
        "  data={}\n",
        "  for file in txt_file:\n",
        "    txt_path=os.path.join(text_dir,file)\n",
        "    text_list=get_text(txt_path)\n",
        "    key_path=os.path.join(key_text_dir,file.replace('.txt','.json'))\n",
        "    #os.rename(key_path,key_path.replace('.txt','.json'))\n",
        "    dic=read_json(key_path)\n",
        "    txt_others=[]\n",
        "    for text in text_list:\n",
        "      for i,(k,v) in enumerate(dic.items()):\n",
        "        if text in v:\n",
        "          data[text]=i+1\n",
        "          break\n",
        "        #elif i==4:\n",
        "          #txt_others.append(text)       \n",
        "    #data[' '.join(txt_others)]=0\n",
        "  save_json('./task3/data.json',data)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SeR-7ybsEkc"
      },
      "source": [
        "encode_label(key_text_dir,text_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiHOwKeNm7k6"
      },
      "source": [
        "data=read_json('./task3/data.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hk58bZZRp9CC"
      },
      "source": [
        "labels=list(data.values())\n",
        "texts=list(data.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkBimZbXqDmI"
      },
      "source": [
        "other_num=labels.count(0)\n",
        "company_num=labels.count(1)\n",
        "data_num=labels.count(2)\n",
        "add_num=labels.count(3)\n",
        "total_num=labels.count(4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeTu4MoUqRF_"
      },
      "source": [
        "print(total_num)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySU0pAxL67s3"
      },
      "source": [
        "# 2. Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcO4h2kzqTi1"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, classes, max_len=32):\n",
        "        self.texts = texts\n",
        "        self.classes = classes\n",
        "        self.max_len = 32\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        text = self.texts[item]\n",
        "        text = tokenizer.tokenize(text)\n",
        "        print(text)\n",
        "        text = tokenizer.encode_plus(text, add_special_tokens=True, padding=True, pad_to_multiple_of=self.max_len)\n",
        "        text_ids = text['input_ids']\n",
        "        text_attn = text['attention_mask']\n",
        "\n",
        "        text_ids = torch.tensor(text_ids, dtype=torch.long)\n",
        "        text_attn = torch.tensor(text_attn, dtype=torch.long)\n",
        "        label = torch.tensor(self.classes[item])\n",
        "\n",
        "        return text_ids, text_attn, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.classes)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDTRhpITAJ7_"
      },
      "source": [
        "from sklearn import model_selection\n",
        "\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(texts, targets, test_size=0.2, random_state=1)\n",
        "\n",
        "train_dataset = TextDataset(X_train, y_train)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJsVedDf_ttm"
      },
      "source": [
        "# 3. Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxBOeSoE_vt2"
      },
      "source": [
        "from transformers import BertModel, BertForSequenceClassification\n",
        "\n",
        "\n",
        "class TextClassification(nn.Module):\n",
        "    def __init__(self, pretrained_name='bert-base-uncased', n_classes=5):\n",
        "        super(TextClassification, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(pretrained_name)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classification = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "\n",
        "    def forward(self, text_ids, text_attns):\n",
        "        out = self.bert(text_ids, attention_mask=text_attns)\n",
        "        out = out.last_hidden_state[:, 0, :]\n",
        "        out = self.dropout(out)\n",
        "        out = self.classification(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dltD_A36B_y5"
      },
      "source": [
        "# 4 Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuADnNitCBKB"
      },
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "model = TextClassification()\n",
        "lr = 1e-4\n",
        "optimizer = Adam(model.parameters(), lr=lr)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "n_epochs = 50\n",
        "\n",
        "losses = []\n",
        "for epoch in range(n_epochs):\n",
        "    print(f'Epoch {epoch + 1}:')\n",
        "\n",
        "    batch_losses = []\n",
        "    for batch_text_ids, batch_text_attns, batch_labels in tqdm(train_dataloader):\n",
        "        batch_text_ids = batch_text_ids.to(device)\n",
        "        batch_text_attns = batch_text_attns.to(device)\n",
        "        batch_labels = torch.flatten(batch_labels).to(device)\n",
        "\n",
        "        pred = model(batch_text_ids, batch_text_attns)\n",
        "        loss = loss_fn(pred, batch_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        print(f'batch loss: {loss.item()}')\n",
        "        batch_losses.append(loss.item())\n",
        "\n",
        "    losses.append(sum(batch_losses) / len(batch_losses))\n",
        "    print(f'Epoch loss: {losses[-1]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKDQ9DriBfQy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}